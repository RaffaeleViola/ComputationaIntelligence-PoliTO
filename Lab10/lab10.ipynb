{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self._board = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "\n",
    "    def add_x(self, pos: tuple[int]):\n",
    "        assert self._board[pos[0]][pos[1]] == 0\n",
    "        self._board[pos[0]][pos[1]] = 1\n",
    "\n",
    "    def add_o(self, pos: tuple[int]):\n",
    "        assert self._board[pos[0]][pos[1]] == 0\n",
    "        self._board[pos[0]][pos[1]] = -1\n",
    "\n",
    "    def check_pos(self, player, pos):\n",
    "        target = 3 if player == \"X\" else -3\n",
    "        center = (1, 1)\n",
    "        right_diag = {(0, 0), (2, 2)}\n",
    "        left_diag = {(0, 2), (2, 0)}\n",
    "        # checking row and column\n",
    "        if self._board[pos[0], :].sum() == target or self._board[:, pos[1]].sum() == target:\n",
    "            return True\n",
    "        # check right_diag\n",
    "        if pos in right_diag or pos == center:\n",
    "            if sum([self._board[dx, dy] for dx, dy in [(0,0), (1, 1), (2, 2)]]) == target:\n",
    "                return True\n",
    "        # check left_diag\n",
    "        if pos in right_diag or pos == center:\n",
    "            if sum([self._board[dx, dy] for dx, dy in [(0,2), (1, 1), (2, 0)]]) == target:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def check(self):\n",
    "        rows = self._board.sum(axis = 0)\n",
    "        cols = self._board.sum(axis = 1)\n",
    "        if(np.any((rows == 3)) or  np.any((cols == 3))\n",
    "                or sum([self._board[dx, dy] for dx, dy in [(0,0), (1, 1), (2, 2)]]) == 3\n",
    "                or sum([self._board[dx, dy] for dx, dy in [(0,2), (1, 1), (2, 0)]]) == 3):\n",
    "            return 1\n",
    "        if(np.any((rows == -3)) or  np.any((cols == -3))\n",
    "                or sum([self._board[dx, dy] for dx, dy in [(0,0), (1, 1), (2, 2)]]) == -3\n",
    "                or sum([self._board[dx, dy] for dx, dy in [(0,2), (1, 1), (2, 0)]]) == -3):\n",
    "            return -1\n",
    "        return 0\n",
    "\n",
    "    def next_moves(self):\n",
    "        return [(i, j) for j in range(3) for i in range(3) if self._board[i, j] == 0]\n",
    "\n",
    "    def __str__(self):\n",
    "        mapping = {0: \"_\", 1: \"X\", -1: \"O\"}\n",
    "        pretty = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                pretty[i][j] = mapping[self._board[i, j]]\n",
    "        return \"\\n\".join([\"|\".join(el) for el in pretty])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[3 3 3]\n",
      "[0 3 6]\n",
      "_|_|_\n",
      "_|_|_\n",
      "_|_|_\n",
      "[(1, 0), (2, 0), (2, 1), (0, 2), (1, 2), (2, 2)]\n"
     ]
    }
   ],
   "source": [
    "board = np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2]])\n",
    "print(board[1, :].sum())\n",
    "print(board.sum(axis = 0))\n",
    "print(board.sum(axis=1))\n",
    "\n",
    "game = TicTacToe()\n",
    "print(game)\n",
    "game.add_o((0, 0))\n",
    "game.add_x((1, 1))\n",
    "game.add_o((0, 1))\n",
    "# game.add_o((0, 1))\n",
    "print(game.next_moves())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def game():\n",
    "    inst = TicTacToe()\n",
    "    print(inst)\n",
    "    player = 0\n",
    "    while not inst.check():\n",
    "        move = input(f\"Player {player} enter move in the form <x-y>\")\n",
    "        xy = move.split(\"-\")\n",
    "        x, y = int(xy[0]), int(xy[1])\n",
    "        if player == 0:\n",
    "            inst.add_x((x, y))\n",
    "        else:\n",
    "            inst.add_o((x, y))\n",
    "        print(inst)\n",
    "        print()\n",
    "        player = 1 - player\n",
    "\n",
    "    print(f\"Player {1 - player} won!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reiforcement Learning Strategy\n",
    "\n",
    "The idea is to use a *Model-Free* Approach\n",
    "\n",
    "Reward = 1 for game won, 0 for draw, -1 for game lost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Q_Table = dict() # it will be updated at runtime\n",
    "learning_rate = 0.0005\n",
    "gamma = 0.002"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in range(n_training_episodes):\n",
    "\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "\n",
    "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "\n",
    "      new_state, reward, done, info = env.step(action)\n",
    "\n",
    "\n",
    "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "      # If done, finish the episode\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "      # Our state is the new state\n",
    "      state = new_state\n",
    "  return Qtable"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
