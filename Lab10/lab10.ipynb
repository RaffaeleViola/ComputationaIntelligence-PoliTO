{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tic Tac Toe Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self._board = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "\n",
    "    def reset(self):\n",
    "        self._board = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "\n",
    "    def add_x(self, pos: tuple[int]):\n",
    "        assert self._board[pos[0]][pos[1]] == 0\n",
    "        self._board[pos[0]][pos[1]] = 1\n",
    "\n",
    "    def add_o(self, pos: tuple[int]):\n",
    "        assert self._board[pos[0]][pos[1]] == 0\n",
    "        self._board[pos[0]][pos[1]] = -1\n",
    "\n",
    "    def check_pos(self, player, pos):\n",
    "        target = 3 if player == \"X\" else -3\n",
    "        center = (1, 1)\n",
    "        right_diag = {(0, 0), (2, 2)}\n",
    "        left_diag = {(0, 2), (2, 0)}\n",
    "        # checking row and column\n",
    "        if self._board[pos[0], :].sum() == target or self._board[:, pos[1]].sum() == target:\n",
    "            return True\n",
    "        # check right_diag\n",
    "        if pos in right_diag or pos == center:\n",
    "            if sum([self._board[dx, dy] for dx, dy in [(0,0), (1, 1), (2, 2)]]) == target:\n",
    "                return True\n",
    "        # check left_diag\n",
    "        if pos in right_diag or pos == center:\n",
    "            if sum([self._board[dx, dy] for dx, dy in [(0,2), (1, 1), (2, 0)]]) == target:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def check(self):\n",
    "        rows = self._board.sum(axis = 0)\n",
    "        cols = self._board.sum(axis = 1)\n",
    "        if(np.any((rows == 3)) or  np.any((cols == 3))\n",
    "                or sum([self._board[dx, dy] for dx, dy in [(0,0), (1, 1), (2, 2)]]) == 3\n",
    "                or sum([self._board[dx, dy] for dx, dy in [(0,2), (1, 1), (2, 0)]]) == 3):\n",
    "            return 1\n",
    "        if(np.any((rows == -3)) or  np.any((cols == -3))\n",
    "                or sum([self._board[dx, dy] for dx, dy in [(0,0), (1, 1), (2, 2)]]) == -3\n",
    "                or sum([self._board[dx, dy] for dx, dy in [(0,2), (1, 1), (2, 0)]]) == -3):\n",
    "            return -1\n",
    "        return 0\n",
    "\n",
    "    def copy(self):\n",
    "        bd_copy = self._board.copy()\n",
    "        new_game = TicTacToe()\n",
    "        new_game._board = bd_copy\n",
    "        return new_game\n",
    "\n",
    "    def next_moves(self):\n",
    "        return [(i, j) for j in range(3) for i in range(3) if self._board[i, j] == 0]\n",
    "\n",
    "    def __str__(self):\n",
    "        mapping = {0: \"_\", 1: \"X\", -1: \"O\"}\n",
    "        pretty = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                pretty[i][j] = mapping[self._board[i, j]]\n",
    "        return \"\\n\".join([\"|\".join(el) for el in pretty])\n",
    "\n",
    "    def new_state(self, move: tuple[int], player):\n",
    "        val = 1 if player == \"X\" else -1\n",
    "        # always player O\n",
    "        board_copy = self._board.copy()\n",
    "        board_copy[move[0], move[1]] = val\n",
    "        return tuple([tuple(row) for row in list(board_copy)])\n",
    "\n",
    "    def get_state(self):\n",
    "         return tuple([tuple(row) for row in list(self._board)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def game():\n",
    "    inst = TicTacToe()\n",
    "    print(inst)\n",
    "    player = 0\n",
    "    while not inst.check():\n",
    "        move = input(f\"Board:\\n{inst}\\n Player {player} enter move in the form <x-y>\")\n",
    "        xy = move.split(\"-\")\n",
    "        x, y = int(xy[0]), int(xy[1])\n",
    "        if player == 0:\n",
    "            inst.add_x((x, y))\n",
    "        else:\n",
    "            inst.add_o((x, y))\n",
    "        print(inst)\n",
    "        print()\n",
    "        player = 1 - player\n",
    "\n",
    "    print(f\"Player {1 - player} won!\")\n",
    "\n",
    "game()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reiforcement Learning Strategy\n",
    "\n",
    "The idea is to use a *Model-Free* Approach\n",
    "\n",
    "Reward = 1 for game won, 0 for draw, -1 for game lost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BackPropagation Strategy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 30_000\n",
    "learning_rate = 0.7\n",
    "\n",
    "# Environment parameters\n",
    "gamma = 0.95\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.005"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "def backpropagation(Qtable: dict, states: list, actions: list, reward):\n",
    "    next_state = None\n",
    "    for act, state in zip(reversed(actions), reversed(states)):\n",
    "        fut_max = max(Qtable[next_state].values()) if next_state else default\n",
    "        Qtable[state][act] = Qtable[state][act] + learning_rate * (reward + gamma * fut_max - Qtable[state][act])\n",
    "        next_state = state\n",
    "    return Qtable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable: dict, state: TicTacToe, epsilon: float):\n",
    "  random_int = random.uniform(0,1)\n",
    "  actions = state.next_moves()\n",
    "  if random_int > epsilon:\n",
    "    return max(actions, key= lambda act: Qtable[state.get_state()][act])\n",
    "  else:\n",
    "    action = actions[random.randint(0, len(actions) - 1)]\n",
    "  return action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training only against Random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:09<00:00, 3044.95it/s]\n"
     ]
    }
   ],
   "source": [
    "Qtable = dict() # {state1: {action1: value, ..., action_n: value} ,..., state_n:{...} }\n",
    "default = 0.0\n",
    "disps = [disp for disp in list(itertools.product([0, 1, -1], repeat = 9)) if -1 <= sum(disp) <= 0] # filter valid states\n",
    "disps = [tuple([disp[ind:ind+3] for ind in range(0, 9, 3)]) for disp in disps]\n",
    "for disp in disps:\n",
    "    next_moves = [(i, j) for j in range(3) for i in range(3) if disp[i][j] == 0]\n",
    "    Qtable[disp] = {mv: default for mv in next_moves}\n",
    "\n",
    "env = TicTacToe()\n",
    "for episode in tqdm(range(n_training_episodes)):\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "    # player = 0 # random.randint(0,1)\n",
    "    for n in [0, 1]:\n",
    "        # Reset the environment\n",
    "        env.reset()\n",
    "        player = n\n",
    "        done = 0\n",
    "        steps = 0\n",
    "        # simulating game\n",
    "        state_action = env.get_state()\n",
    "        state_history = [state_action] if n == 1 else []\n",
    "        actions_history = []\n",
    "        while done == 0 and steps <9:\n",
    "            actions = env.next_moves()\n",
    "            if player == 1:\n",
    "                action = epsilon_greedy_policy(Qtable, env, epsilon)\n",
    "                env.add_x(action)\n",
    "                actions_history.append(action)\n",
    "            else:\n",
    "                action = actions[random.randint(0, len(actions) - 1)]\n",
    "                env.add_o(action)\n",
    "                state_history.append(env.get_state())\n",
    "            done = env.check()\n",
    "            player = 1 - player\n",
    "            steps += 1\n",
    "        # updating states\n",
    "        reward = done if done != 0 else 0.05 # +1 win -1 lose 0.1 draw\n",
    "        if len(state_history) != len(actions_history):\n",
    "            # print(state_history)\n",
    "            # print(actions_history)\n",
    "            state_history.pop()\n",
    "        Qtable = backpropagation(Qtable, state_history, actions_history, reward)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training only with random is not perfectly effective because the training phase tends to not explore all the possible states\n",
    "\n",
    "and in model free approach, that is deterministic, this leads to weaknesses of the agent trained. Being tictactoe an easy game\n",
    "with few states, it is easy to find the weakness by playing against the agent 20-30 times.\n",
    "\n",
    "We need to find a better trainer!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MinMax Strategy as benchmark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "min_cache = {}\n",
    "max_cache = {}\n",
    "def minmax(game: TicTacToe, plyr, to_maximize):\n",
    "    cache = max_cache if plyr == to_maximize else min_cache\n",
    "    if (game.get_state(), plyr) in cache:\n",
    "        return cache[(game.get_state(), plyr)]\n",
    "    over = game.check()\n",
    "    next_actions = game.next_moves()\n",
    "    if over != 0 or not next_actions:\n",
    "        if to_maximize == \"O\":\n",
    "            over = -over\n",
    "        # cache[game.get_state()] = (None, over)\n",
    "        return None, over\n",
    "    scores = []\n",
    "    for i, move in enumerate(next_actions):\n",
    "        new_g = game.copy()\n",
    "        if plyr == \"X\":\n",
    "            new_g.add_x(move)\n",
    "            scores.append((i, minmax(new_g, \"O\", to_maximize)[1]))\n",
    "        else:\n",
    "            new_g.add_o(move)\n",
    "            scores.append((i, minmax(new_g, \"X\", to_maximize)[1]))\n",
    "    if plyr == to_maximize: # max player\n",
    "        best = max(scores, key= lambda x: x[1])\n",
    "    else:\n",
    "        best = min(scores, key = lambda  x: x[1])\n",
    "    choice = next_actions[best[0]]\n",
    "    cache[(game.get_state(), plyr)] = (choice, best[1])\n",
    "    return choice, best[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [],
   "source": [
    "initial_moves = [(i, j) for i in range(3) for j in range(3)]\n",
    "gm = TicTacToe()\n",
    "# fill memoization\n",
    "minmax(gm, \"X\", \"X\")\n",
    "for mv in initial_moves:\n",
    "    gm_copy = gm.copy().add_o(mv)\n",
    "    minmax(gm, \"X\", \"X\")\n",
    "\n",
    "minmax(gm, \"O\", \"O\")\n",
    "for mv in initial_moves:\n",
    "    gm_copy = gm.copy().add_x(mv)\n",
    "    minmax(gm, \"O\", \"O\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, name, letter):\n",
    "        self._name = name\n",
    "        self._letter = letter\n",
    "\n",
    "    def get_name(self):\n",
    "        return self._name\n",
    "\n",
    "    def move(self, game_inst: TicTacToe):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def letter(self):\n",
    "        return self._letter\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "class MinMaxPlayer(Player):\n",
    "    def __init__(self, name, letter):\n",
    "        super().__init__(name, letter)\n",
    "\n",
    "    def get_name(self):\n",
    "        return self._name\n",
    "\n",
    "    def move(self, game_inst: TicTacToe):\n",
    "        if self._letter == \"X\":\n",
    "            mv, _ = minmax(game_inst, \"X\", \"X\")\n",
    "            game_inst.add_x(mv)\n",
    "        else:\n",
    "            mv, _ = minmax(game_inst, \"O\", \"O\")\n",
    "            game_inst.add_o(mv)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "class RandomPlayer(Player):\n",
    "    def __init__(self, name, letter):\n",
    "        super().__init__(name, letter)\n",
    "\n",
    "    def get_name(self):\n",
    "        return self._name\n",
    "\n",
    "    def move(self, game_inst: TicTacToe):\n",
    "        if self._letter == \"X\":\n",
    "            acts = game_inst.next_moves()\n",
    "            mv = random.randint(0, len(acts) - 1)\n",
    "            game_inst.add_x(acts[mv])\n",
    "        else:\n",
    "            acts = game_inst.next_moves()\n",
    "            mv = random.randint(0, len(acts) - 1)\n",
    "            game_inst.add_o(acts[mv])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "class QPlayer(Player):\n",
    "    def __init__(self, name, qtable: dict, letter):\n",
    "        super().__init__(name, letter)\n",
    "        self._QTable = qtable\n",
    "\n",
    "    def get_name(self):\n",
    "        return self._name\n",
    "\n",
    "    def move(self, game_inst: TicTacToe):\n",
    "        acts = game_inst.next_moves()\n",
    "        if self._letter == \"X\":\n",
    "            best_move = max(acts, key=lambda act: Qtable[game_inst.get_state()][act])\n",
    "        else:\n",
    "            # Tha agent learned to play with +1 strategy => we need to invert the state to properly query the QTable\n",
    "            def invert_state(state):\n",
    "                brd = []\n",
    "                for i in range(3):\n",
    "                    brd.append([])\n",
    "                    for el in state[i]:\n",
    "                        brd[i].append(-el)\n",
    "                brd = tuple([tuple(row) for row in brd])\n",
    "                return brd\n",
    "            best_move = max(acts, key=lambda act: Qtable[invert_state(game_inst.get_state())][act])\n",
    "        if self._letter == \"X\":\n",
    "            game_inst.add_x(best_move)\n",
    "        else:\n",
    "            game_inst.add_o(best_move)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:11<00:00, 2649.25it/s]\n"
     ]
    }
   ],
   "source": [
    "Qtable = dict() # {state1: {action1: value, ..., action_n: value} ,..., state_n:{...} }\n",
    "default = 0.0\n",
    "disps = [disp for disp in list(itertools.product([0, 1, -1], repeat = 9)) if -1 <= sum(disp) <= 0] # filter valid states\n",
    "disps = [tuple([disp[ind:ind+3] for ind in range(0, 9, 3)]) for disp in disps]\n",
    "for disp in disps:\n",
    "    next_moves = [(i, j) for j in range(3) for i in range(3) if disp[i][j] == 0]\n",
    "    Qtable[disp] = {mv: default for mv in next_moves}\n",
    "\n",
    "env = TicTacToe()\n",
    "min_max_player = MinMaxPlayer(\"Trainer\", \"O\")\n",
    "random_player = RandomPlayer(\"RandomTrainer\", \"O\")\n",
    "\n",
    "for episode in tqdm(range(30_000)):\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "    # player = 0 # random.randint(0,1)\n",
    "    for n in [0, 1]:\n",
    "        min_max_agent = random.choice([False, False, True]) # minmax 25%\n",
    "        # Reset the environment\n",
    "        env.reset()\n",
    "        player = n\n",
    "        done = 0\n",
    "        steps = 0\n",
    "        # simulating game\n",
    "        state_action = env.get_state()\n",
    "        state_history = [state_action] if n == 1 else []\n",
    "        actions_history = []\n",
    "        while done == 0 and steps <9:\n",
    "            actions = env.next_moves()\n",
    "            if player == 1:\n",
    "                action = epsilon_greedy_policy(Qtable, env, epsilon)\n",
    "                env.add_x(action)\n",
    "                actions_history.append(action)\n",
    "            else:\n",
    "                if not min_max_agent or steps == 0:\n",
    "                    random_player.move(env)\n",
    "                else:\n",
    "                    action = min_max_player.move(env)\n",
    "                state_history.append(env.get_state())\n",
    "            done = env.check()\n",
    "            player = 1 - player\n",
    "            steps += 1\n",
    "        # updating states\n",
    "        reward = done if done != 0 else 0.00 # +1 win -1 lose 0.1 draw\n",
    "        if len(state_history) != len(actions_history):\n",
    "            state_history.pop()\n",
    "        Qtable = backpropagation(Qtable, state_history, actions_history, reward)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "perfectX_player = QPlayer(\"PerfectX Player\", Qtable, \"X\")\n",
    "perfectO_player = QPlayer(\"PerfectO Player\", Qtable, \"O\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "def benchmark(player1: Player, player2: Player):\n",
    "    n_games = 10_000\n",
    "    winning = 0\n",
    "    drawing = 0\n",
    "    # first needs to be random otherwise minmax plays everytime the same strategy\n",
    "    winner = 1 if player1.letter == \"X\" else -1\n",
    "    rndO = RandomPlayer(\"OPlayer\", \"O\")\n",
    "    rndX = RandomPlayer(\"XPlayer\", \"X\")\n",
    "    first_move = {\"O\": rndO, \"X\": rndX}\n",
    "    for _ in tqdm(range(n_games)):\n",
    "        inst = TicTacToe()\n",
    "        player = 0\n",
    "        steps = 0\n",
    "        while inst.check() == 0 and steps < 9:\n",
    "            if player == 0:\n",
    "                if steps == 0:\n",
    "                    first_move[player1.letter].move(inst)\n",
    "                else:\n",
    "                    player1.move(inst)\n",
    "            else:\n",
    "                if steps == 0:\n",
    "                    first_move[player2.letter].move(inst)\n",
    "                else:\n",
    "                    player2.move(inst)\n",
    "            player = 1 - player\n",
    "            steps += 1\n",
    "        if inst.check() == winner:\n",
    "            winning += 1\n",
    "        elif inst.check() == 0:\n",
    "            drawing += 1\n",
    "    win_rate = winning * 100 / n_games\n",
    "    draw_rate = drawing * 100 / n_games\n",
    "    print(f\"{player1.name} winning Rate against {player2.name}: {win_rate}%\")\n",
    "    print(f\"{player1.name} Drawing Rate against {player2.name}: {draw_rate}%\")\n",
    "    print(f\"{player1.name} Losing Rate against {player2.name}: {100 - win_rate - draw_rate}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4762.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerfectX Player winning Rate against MinMax: 0.0%\n",
      "PerfectX Player Drawing Rate against MinMax: 100.0%\n",
      "PerfectX Player Losing Rate against MinMax: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4588.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax winning Rate against PerfectX Player: 0.0%\n",
      "MinMax Drawing Rate against PerfectX Player: 100.0%\n",
      "MinMax Losing Rate against PerfectX Player: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 6607.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerfectX Player winning Rate against Random: 86.28%\n",
      "PerfectX Player Drawing Rate against Random: 11.04%\n",
      "PerfectX Player Losing Rate against Random: 2.6799999999999997%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 5957.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random winning Rate against PerfectX Player: 0.89%\n",
      "Random Drawing Rate against PerfectX Player: 18.38%\n",
      "Random Losing Rate against PerfectX Player: 80.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3969.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerfectO Player winning Rate against PerfectX Player: 11.15%\n",
      "PerfectO Player Drawing Rate against PerfectX Player: 88.85%\n",
      "PerfectO Player Losing Rate against PerfectX Player: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3880.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerfectX Player winning Rate against PerfectO Player: 11.26%\n",
      "PerfectX Player Drawing Rate against PerfectO Player: 88.74%\n",
      "PerfectX Player Losing Rate against PerfectO Player: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "min_max_player = MinMaxPlayer(\"MinMax\", \"O\")\n",
    "random_player = RandomPlayer(\"Random\", \"O\")\n",
    "benchmark(perfectX_player, min_max_player)\n",
    "benchmark(min_max_player, perfectX_player)\n",
    "benchmark(perfectX_player, random_player)\n",
    "benchmark(random_player, perfectX_player)\n",
    "benchmark(perfectO_player, perfectX_player)\n",
    "benchmark(perfectX_player, perfectO_player)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# It's time to play\n",
    "Change the variable \"player\" to 1 to play the second turn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def game_again_agent():\n",
    "    inst = TicTacToe()\n",
    "    print(inst)\n",
    "    player = 0\n",
    "    agent = perfectX_player\n",
    "    steps = 0\n",
    "    while inst.check() == 0 and steps < 9:\n",
    "        if player == 0:\n",
    "            move = input(f\"Board:\\n{inst}\\n Player {player} enter move in the form <x-y>\")\n",
    "            xy = move.split(\"-\")\n",
    "            x, y = int(xy[0]), int(xy[1])\n",
    "            inst.add_o((x, y))\n",
    "        else:\n",
    "            agent.move(inst)\n",
    "        print(inst)\n",
    "        print()\n",
    "        player = 1 - player\n",
    "        steps += 1\n",
    "    if inst.check() != 0:\n",
    "        print(f\"Player {1 - player} won!\")\n",
    "    else:\n",
    "        print(f\"It's a draw guys!\")\n",
    "\n",
    "game_again_agent()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
