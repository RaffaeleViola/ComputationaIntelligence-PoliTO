
## Quixo Exam - Raffaele Viola s309063

### Agent proposed 
The agent is proposed through the class MyPlayer. It combines two different agents under the hood, as explained below.
On average, it reaches more than 95% victories against a random player. 

Given that there is not a predefined use case of the agent, I do not manage the cache of the player and it can grow 
without limits. Thus do not make it playing hundreds of thousands games (if you need it you can recreate another object).

Project organization:
- mimax.py -> MinMax agent
- monteCarloTreeSearch.py -> MCTS agent
- utils.py -> utility functions to generate next actions and check if they are acceptable given the current game state
- qtable.py/trainingQlearning.py -> Qlearning implementation but the agent is not delivered for the exam, just if you
want to take a look at it

### MinMax
* Implemented MinMax Strategy with a limited depth considering the large search space 
* Implemented alpha-beta pruning to reduce the branching factor
* Added a cache to speed up the MinMax response and not recompute all the tree at every move (the cache considers also
 the depth)
* Implemented rotations of the table in order to reduce more the branching factor
* Firstly the score system was standard (+1 win, -1 lose, 0 draw) but I noticed 
 that the model was unable to learn something useful in the first stages of the game. I thus implemented a score function
 that consists in the sum of the maximum row, column and diagonal density.

```

def scoring_fun(game: Game, player: int, to_maximize: int):
    brd = game.get_board()
    max_rows = max([(brd[i, :] == player).sum() for i in range(5)])
    max_columns = max([(brd[:, i] == player).sum() for i in range(5)])
    diag1 = sum([(brd[i, i] == player).sum() for i in range(5)])
    diag2 = sum([(brd[i, j] == player).sum() for i, j in zip(range(5), range(4, -1, -1))])
    mapping = {0: 0, 1: 1, 2: 10, 3: 100, 4: 1000, 5: 10_000}
    if player == to_maximize:
        return mapping[max_rows] + mapping[max_columns] + mapping[diag1] + mapping[diag2]
    else:
        return -(mapping[max_rows] + mapping[max_columns] + mapping[diag1] + mapping[diag2])

```

### QLearning Tentative
* I tried to implement a QLearning approach but even making it training for long time led me to a quite random strategy.
It was a "brute force" approach, with no particular design of the data structure (a dict object) and of the serialization.
I think that the best way to improve it would have been to implement all the possible symmetries and rotations in order
to reduce the search space and store the boards considering 2 bits for every element in the matrix (there are 3
possibilities) and  4 + 2 bits for a single action (16 positions on the perimeter and 4 possible directions).
I left the code on GitHub, but I did not create an agent for the exam because I moved to the evaluation of the MCTS approach, that 
seemed to me more promising and less memory consuming. The Qlearning implemented do not use Deep Learning.

### Montecarlo Tree Search
* I explored and implemented the Montecarlo Tree Search. I implemented a Node class to represent a state generated 
 by a given player (Important because the same state can be generated by both players). 

```
    def __init__(self, board: np.ndarray, player: int, parent=None):
        self.state = board.copy()
        self.player = player  # the player that generated the current board
        self.wins = 0
        self.draws = 0
        self.visits = 0
        self.parent = parent
        tmp_game = Game()
        tmp_game._board = board.copy()
        tmp_game.current_player_idx = player
        self.over = tmp_game.check_winner()
        tmp_game.current_player_idx = (player + 1) % 2
        self.next_moves = next_accetptable_moves(tmp_game, (player + 1) % 2)
        self.childs: dict[Node, Move] = dict()
```

* I apply the Selection - Expansion - Simulation - Backpropagation 100 times and the simulation is performed over 70 games.
* The selection is performed by computing the Upper Confidence Bound (UCB) and then taking the child node with the highest value.
  I found this metric in an article on Medium blog. Selection stops when it reaches a leaf node.

```
      def calculate_ucb(self) -> float:
        if self.visits == 0:
            return math.inf
        parent_node_visits = self.parent.visits
        exploration_term = (math.sqrt(2.0) * math.sqrt(math.log(parent_node_visits) / self.visits))
        value = self.value() + exploration_term
        return value
```

* The expansion is performed by selecting randomly one of the possible moves available from the previously selected node
 and creating the new child node.
* The simulation is performed by playing 70 random games and computing the statistics: win and draw rate. In my game 
implementation a draw is when 100 total moves are reached.
* The backpropagation is performed by propagating the previous statistics, following the parent pointer in the nodes.
* Given the fact that the search space is not a tree because some boards can be recreated during the game (there are 
cycles in the tree), I have added a global cache used in the selection phase to start from an already visited state as root
(if present), and a local cache that avoids to get stuck in a cycle while visiting the tree towards a leaf.

### Agents Fused
* I observed that:
  * MCTS is really slow by construction, and It is not effective in the first stages of the game.
  * MinMax is really effective but conservative, thus in some games his strategy results not really "aggressive" and this 
  may lead to losing the game
* In order to enhance the overall performance I mixed the 2 agents. Most of the time the player is MinMax but if the 
board has few free spots (the strategy is too conservative) or the MinMax plays a move that leads the opponent to 
victory, the move is performed by MCTS. It is important to say that MinMax is depth-limited, thus it may end up in a 
situation where every move leads the opponent to victory (for example the "doppio gioco" as in Tic Tac Toe). If this 
case happens, the MinMax player will perform the move that makes the opponent winning and to avoid this behaviour I perform
the move with the MCTS player.